{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/shakespeare.txt'\n",
    "sentences = []\n",
    "all_chars = []\n",
    "with open(fn, 'r') as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 3:\n",
    "            if line[:2] == '  ': line = line[2:]\n",
    "            sentences.append(line.lower())\n",
    "            all_chars += list(line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 93673\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary.\n",
    "chars = sorted(list(set(all_chars)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(all_chars)\n",
    "n_vocab = len(chars)\n",
    "print('Total characters: %d' % n_chars)\n",
    "print('Total vocab: %d' % n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 31211\n"
     ]
    }
   ],
   "source": [
    "# Construct dataset.\n",
    "seq_len = 40\n",
    "step_size = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_len, step_size):\n",
    "    seq_in = all_chars[i: i + seq_len]\n",
    "    seq_out = all_chars[i + seq_len]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print('Total patterns: %d' % n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_len, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bfeng/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               161600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 169,238\n",
      "Trainable params: 169,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dim = 200\n",
    "model = Sequential()\n",
    "model.add(LSTM(dim, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'lstm_weights/best_model\n",
    ".h5' % dim\n",
    "mc = ModelCheckpoint(\n",
    "    'lstm_weights/best_model_dim-%d.h5' % dim, monitor='acc', save_best_only=True)\n",
    "es = EarlyStopping(monitor='acc', baseline=0.6, patience=0)\n",
    "callbacks_list = [mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "31211/31211 [==============================] - 13s 423us/step - loss: 1.2783 - acc: 0.6019\n",
      "Epoch 77/200\n",
      "31211/31211 [==============================] - 14s 441us/step - loss: 1.2739 - acc: 0.6006\n",
      "Epoch 78/200\n",
      "31211/31211 [==============================] - 14s 436us/step - loss: 1.3241 - acc: 0.5865\n",
      "Epoch 79/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 1.2586 - acc: 0.6054\n",
      "Epoch 80/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 1.2314 - acc: 0.6144\n",
      "Epoch 81/200\n",
      "31211/31211 [==============================] - 13s 429us/step - loss: 1.2212 - acc: 0.6197\n",
      "Epoch 82/200\n",
      "31211/31211 [==============================] - 14s 434us/step - loss: 1.2377 - acc: 0.6140\n",
      "Epoch 83/200\n",
      "31211/31211 [==============================] - 13s 431us/step - loss: 1.2305 - acc: 0.6130\n",
      "Epoch 84/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 1.2149 - acc: 0.6181\n",
      "Epoch 85/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 1.2925 - acc: 0.5948\n",
      "Epoch 86/200\n",
      "31211/31211 [==============================] - 14s 434us/step - loss: 1.2324 - acc: 0.6135\n",
      "Epoch 87/200\n",
      "31211/31211 [==============================] - 13s 428us/step - loss: 1.1961 - acc: 0.6247\n",
      "Epoch 88/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 1.1998 - acc: 0.6253\n",
      "Epoch 89/200\n",
      "31211/31211 [==============================] - 13s 431us/step - loss: 1.2300 - acc: 0.6125\n",
      "Epoch 90/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 1.1644 - acc: 0.6352\n",
      "Epoch 91/200\n",
      "31211/31211 [==============================] - 13s 429us/step - loss: 1.1569 - acc: 0.6369\n",
      "Epoch 92/200\n",
      "31211/31211 [==============================] - 14s 444us/step - loss: 1.1581 - acc: 0.6334\n",
      "Epoch 93/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 1.1631 - acc: 0.6344\n",
      "Epoch 94/200\n",
      "31211/31211 [==============================] - 13s 431us/step - loss: 1.1231 - acc: 0.6478\n",
      "Epoch 95/200\n",
      "31211/31211 [==============================] - 13s 424us/step - loss: 1.1803 - acc: 0.6296\n",
      "Epoch 96/200\n",
      "31211/31211 [==============================] - 13s 420us/step - loss: 1.1847 - acc: 0.6262\n",
      "Epoch 97/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 1.1298 - acc: 0.6427\n",
      "Epoch 98/200\n",
      "31211/31211 [==============================] - 13s 429us/step - loss: 1.0885 - acc: 0.6564\n",
      "Epoch 99/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 1.1040 - acc: 0.6512\n",
      "Epoch 100/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 1.1069 - acc: 0.6530\n",
      "Epoch 101/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 1.0972 - acc: 0.6536\n",
      "Epoch 102/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 1.1137 - acc: 0.6467\n",
      "Epoch 103/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 1.1117 - acc: 0.6468\n",
      "Epoch 104/200\n",
      "31211/31211 [==============================] - 14s 448us/step - loss: 1.1046 - acc: 0.6517\n",
      "Epoch 105/200\n",
      "31211/31211 [==============================] - 14s 449us/step - loss: 1.1348 - acc: 0.6392\n",
      "Epoch 106/200\n",
      "31211/31211 [==============================] - 14s 450us/step - loss: 1.0751 - acc: 0.6621\n",
      "Epoch 107/200\n",
      "31211/31211 [==============================] - 14s 445us/step - loss: 1.0535 - acc: 0.6677\n",
      "Epoch 108/200\n",
      "31211/31211 [==============================] - 14s 441us/step - loss: 1.0708 - acc: 0.6621\n",
      "Epoch 109/200\n",
      "31211/31211 [==============================] - 14s 437us/step - loss: 1.1030 - acc: 0.6522\n",
      "Epoch 110/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 1.0632 - acc: 0.6628\n",
      "Epoch 111/200\n",
      "31211/31211 [==============================] - 14s 436us/step - loss: 1.0744 - acc: 0.6633\n",
      "Epoch 112/200\n",
      "31211/31211 [==============================] - 14s 441us/step - loss: 1.6225 - acc: 0.5296\n",
      "Epoch 113/200\n",
      "31211/31211 [==============================] - 14s 437us/step - loss: 1.2841 - acc: 0.5953\n",
      "Epoch 114/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 1.0727 - acc: 0.6613\n",
      "Epoch 115/200\n",
      "31211/31211 [==============================] - 13s 431us/step - loss: 1.0349 - acc: 0.6738\n",
      "Epoch 116/200\n",
      "31211/31211 [==============================] - 14s 436us/step - loss: 1.0175 - acc: 0.6780\n",
      "Epoch 117/200\n",
      "31211/31211 [==============================] - 13s 426us/step - loss: 1.0408 - acc: 0.6710\n",
      "Epoch 118/200\n",
      "31211/31211 [==============================] - 13s 425us/step - loss: 1.0408 - acc: 0.6711\n",
      "Epoch 119/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 1.0187 - acc: 0.6800\n",
      "Epoch 120/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 1.0771 - acc: 0.6588\n",
      "Epoch 121/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 1.0386 - acc: 0.6687\n",
      "Epoch 122/200\n",
      "31211/31211 [==============================] - 14s 440us/step - loss: 1.0235 - acc: 0.6762\n",
      "Epoch 123/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 1.0383 - acc: 0.6713\n",
      "Epoch 124/200\n",
      "31211/31211 [==============================] - 13s 428us/step - loss: 0.9826 - acc: 0.6897\n",
      "Epoch 125/200\n",
      "31211/31211 [==============================] - 14s 434us/step - loss: 1.0354 - acc: 0.6730\n",
      "Epoch 126/200\n",
      "31211/31211 [==============================] - 14s 444us/step - loss: 1.0012 - acc: 0.6823\n",
      "Epoch 127/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 0.9994 - acc: 0.6807\n",
      "Epoch 128/200\n",
      "31211/31211 [==============================] - 14s 444us/step - loss: 0.9588 - acc: 0.6968\n",
      "Epoch 129/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 1.0761 - acc: 0.6590\n",
      "Epoch 130/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 0.9981 - acc: 0.6838\n",
      "Epoch 131/200\n",
      "31211/31211 [==============================] - 14s 443us/step - loss: 0.9804 - acc: 0.6899\n",
      "Epoch 132/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 0.9392 - acc: 0.7061\n",
      "Epoch 133/200\n",
      "31211/31211 [==============================] - 14s 448us/step - loss: 0.9313 - acc: 0.7072\n",
      "Epoch 134/200\n",
      "31211/31211 [==============================] - 14s 442us/step - loss: 0.9450 - acc: 0.7020\n",
      "Epoch 135/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 0.9442 - acc: 0.6963\n",
      "Epoch 136/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.9235 - acc: 0.7037\n",
      "Epoch 137/200\n",
      "31211/31211 [==============================] - 15s 472us/step - loss: 0.9254 - acc: 0.7060\n",
      "Epoch 138/200\n",
      "31211/31211 [==============================] - 14s 449us/step - loss: 0.9864 - acc: 0.6860\n",
      "Epoch 139/200\n",
      "31211/31211 [==============================] - 14s 443us/step - loss: 0.9620 - acc: 0.6919\n",
      "Epoch 140/200\n",
      "31211/31211 [==============================] - 14s 434us/step - loss: 0.9178 - acc: 0.7082\n",
      "Epoch 141/200\n",
      "31211/31211 [==============================] - 14s 443us/step - loss: 0.9419 - acc: 0.7013\n",
      "Epoch 142/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 0.8965 - acc: 0.7176\n",
      "Epoch 143/200\n",
      "31211/31211 [==============================] - 14s 447us/step - loss: 0.9306 - acc: 0.7056\n",
      "Epoch 144/200\n",
      "31211/31211 [==============================] - 14s 442us/step - loss: 0.9149 - acc: 0.7076\n",
      "Epoch 145/200\n",
      "31211/31211 [==============================] - 14s 434us/step - loss: 0.9381 - acc: 0.7034\n",
      "Epoch 146/200\n",
      "31211/31211 [==============================] - 14s 442us/step - loss: 0.8852 - acc: 0.7167\n",
      "Epoch 147/200\n",
      "31211/31211 [==============================] - 14s 441us/step - loss: 0.9448 - acc: 0.6989\n",
      "Epoch 148/200\n",
      "31211/31211 [==============================] - 14s 444us/step - loss: 0.9648 - acc: 0.6929\n",
      "Epoch 149/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 0.9449 - acc: 0.6975\n",
      "Epoch 150/200\n",
      "31211/31211 [==============================] - 14s 445us/step - loss: 0.8658 - acc: 0.7229\n",
      "Epoch 151/200\n",
      "31211/31211 [==============================] - 14s 436us/step - loss: 0.8803 - acc: 0.7201\n",
      "Epoch 152/200\n",
      "31211/31211 [==============================] - 14s 449us/step - loss: 0.9224 - acc: 0.7081\n",
      "Epoch 153/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.9602 - acc: 0.6943\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 14s 434us/step - loss: 0.9015 - acc: 0.7138\n",
      "Epoch 155/200\n",
      "31211/31211 [==============================] - 14s 456us/step - loss: 0.8842 - acc: 0.7172\n",
      "Epoch 156/200\n",
      "31211/31211 [==============================] - 15s 474us/step - loss: 0.8941 - acc: 0.7171\n",
      "Epoch 157/200\n",
      "31211/31211 [==============================] - 14s 437us/step - loss: 0.9409 - acc: 0.6999\n",
      "Epoch 158/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.8795 - acc: 0.7213\n",
      "Epoch 159/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 0.8648 - acc: 0.7219\n",
      "Epoch 160/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 0.8231 - acc: 0.7379\n",
      "Epoch 161/200\n",
      "31211/31211 [==============================] - 13s 425us/step - loss: 0.8580 - acc: 0.7263\n",
      "Epoch 162/200\n",
      "31211/31211 [==============================] - 13s 429us/step - loss: 0.8726 - acc: 0.7228\n",
      "Epoch 163/200\n",
      "31211/31211 [==============================] - 13s 428us/step - loss: 0.8350 - acc: 0.7339\n",
      "Epoch 164/200\n",
      "31211/31211 [==============================] - 13s 427us/step - loss: 0.8460 - acc: 0.7296\n",
      "Epoch 165/200\n",
      "31211/31211 [==============================] - 14s 434us/step - loss: 0.8436 - acc: 0.7296\n",
      "Epoch 166/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 0.9280 - acc: 0.7024\n",
      "Epoch 167/200\n",
      "31211/31211 [==============================] - 13s 431us/step - loss: 0.8540 - acc: 0.7276\n",
      "Epoch 168/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 0.8322 - acc: 0.7378\n",
      "Epoch 169/200\n",
      "31211/31211 [==============================] - 14s 433us/step - loss: 0.8260 - acc: 0.7371\n",
      "Epoch 170/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 0.8229 - acc: 0.7374\n",
      "Epoch 171/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 0.8195 - acc: 0.7408\n",
      "Epoch 172/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 0.8662 - acc: 0.7276\n",
      "Epoch 173/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.8536 - acc: 0.7262\n",
      "Epoch 174/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 0.8165 - acc: 0.7407\n",
      "Epoch 175/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 0.8396 - acc: 0.7313\n",
      "Epoch 176/200\n",
      "31211/31211 [==============================] - 13s 427us/step - loss: 0.8337 - acc: 0.7342\n",
      "Epoch 177/200\n",
      "31211/31211 [==============================] - 13s 421us/step - loss: 0.8379 - acc: 0.7324\n",
      "Epoch 178/200\n",
      "31211/31211 [==============================] - 13s 424us/step - loss: 0.7977 - acc: 0.7452\n",
      "Epoch 179/200\n",
      "31211/31211 [==============================] - 13s 421us/step - loss: 0.8197 - acc: 0.7377\n",
      "Epoch 180/200\n",
      "31211/31211 [==============================] - 13s 422us/step - loss: 0.8230 - acc: 0.7381\n",
      "Epoch 181/200\n",
      "31211/31211 [==============================] - 13s 429us/step - loss: 0.8083 - acc: 0.7412\n",
      "Epoch 182/200\n",
      "31211/31211 [==============================] - 13s 430us/step - loss: 0.8080 - acc: 0.7422\n",
      "Epoch 183/200\n",
      "31211/31211 [==============================] - 13s 427us/step - loss: 0.8293 - acc: 0.7360\n",
      "Epoch 184/200\n",
      "31211/31211 [==============================] - 13s 428us/step - loss: 0.7821 - acc: 0.7477\n",
      "Epoch 185/200\n",
      "31211/31211 [==============================] - 14s 433us/step - loss: 0.7774 - acc: 0.7527\n",
      "Epoch 186/200\n",
      "31211/31211 [==============================] - 14s 437us/step - loss: 0.8379 - acc: 0.7306\n",
      "Epoch 187/200\n",
      "31211/31211 [==============================] - 13s 432us/step - loss: 0.8282 - acc: 0.7346\n",
      "Epoch 188/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.8721 - acc: 0.7187\n",
      "Epoch 189/200\n",
      "31211/31211 [==============================] - 14s 435us/step - loss: 0.8277 - acc: 0.7335\n",
      "Epoch 190/200\n",
      "31211/31211 [==============================] - 14s 438us/step - loss: 0.7628 - acc: 0.7549\n",
      "Epoch 191/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.7514 - acc: 0.7613\n",
      "Epoch 192/200\n",
      "31211/31211 [==============================] - 14s 440us/step - loss: 0.7750 - acc: 0.7536\n",
      "Epoch 193/200\n",
      "31211/31211 [==============================] - 14s 439us/step - loss: 0.8291 - acc: 0.7350\n",
      "Epoch 194/200\n",
      "31211/31211 [==============================] - 14s 443us/step - loss: 0.7510 - acc: 0.7618\n",
      "Epoch 195/200\n",
      "31211/31211 [==============================] - 14s 458us/step - loss: 0.7797 - acc: 0.7502\n",
      "Epoch 196/200\n",
      "31211/31211 [==============================] - 14s 451us/step - loss: 0.8364 - acc: 0.7296\n",
      "Epoch 197/200\n",
      "31211/31211 [==============================] - 14s 451us/step - loss: 0.8351 - acc: 0.7353\n",
      "Epoch 198/200\n",
      "31211/31211 [==============================] - 14s 449us/step - loss: 0.7520 - acc: 0.7606\n",
      "Epoch 199/200\n",
      "31211/31211 [==============================] - 14s 461us/step - loss: 0.7308 - acc: 0.7651\n",
      "Epoch 200/200\n",
      "31211/31211 [==============================] - 14s 446us/step - loss: 0.8081 - acc: 0.7406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff730cc9d68>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list, initial_epoch=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # Helper function to sample from a probability array\n",
    "    # Source: https://keras.io/examples/lstm_text_generation/\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 7s 226us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6564503235568844, 0.7918041944503784]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = \"lstm_weights/best_model_dim-%d.h5\" % dim\n",
    "model.load_weights(ckpt)\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "pattern = [char_to_int[char] for char in seed]\n",
    "output_seq = pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 1.0\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    #index = sample(prediction, temperature=tmp)\n",
    "    index = np.argmax(prediction)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:]\n",
    "    output_seq.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "tthos aaiut mr haart in thls thruldst,\n",
      "io ttcv gile tonl cidutee't cum for the sreeser ofrhohgs bdyint sh thene,\n",
      "wiich bllnger she shaho, when sookts uhou art alvh mfnd own lose,\n",
      "wiech nno mieyhnl au dnrld io theme bgnrned\n",
      "oo thee,\n",
      "to mons of lensr in the grared bod shee.\n",
      "when sookir't sooeu caste ost live iy bs g lov medru\n",
      "cyp his bdcrhe,\n",
      "and hos lemea bla esrh laspen bs darphlc oossoen bsleg,\n",
      "thene bllpe ho the faatini oagenad,\n",
      "then thou iolwinla bnd wht aets mn theer gour facr stame.\n",
      "bnd wou and oanedr thmu tha wirrh soeil,\n",
      "fne poicren, bnd shmu art io thy beutrees bat iise,\n",
      "and sha derter as iy soaiterg sioguw\n",
      "shael non.\n",
      "thel thet makd that thec foieers our pfgwmyh,\n",
      "that shmuld yhth mooe aeloneit amun,\n",
      "orrt shat ph your frart loot dnthwued shre,\n",
      "iow maty ay thes mare bla oae cnoteerted brd shee.\n",
      "when sookei's kunw thou mott laee then i poeide yith that is ulge\n",
      "iilvllg ladk,\n",
      "the bglogn that ceiuiy ceauty shoulds delrh.\n",
      "and for the gaglpred ios thy shose,\n",
      "no let soaven'o dra nf thes i\n"
     ]
    }
   ],
   "source": [
    "output = [int_to_char[i] for i in output_seq]\n",
    "output = ''.join(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31211/31211 [==============================] - 7s 237us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2026776160613595, 0.6283681988716125]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
