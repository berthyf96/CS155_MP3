{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/shakespeare.txt'\n",
    "sentences = []\n",
    "all_chars = []\n",
    "with open(fn, 'r') as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 3:\n",
    "            if line[:2] == '  ': line = line[2:]\n",
    "            sentences.append(line.lower())\n",
    "            all_chars += list(line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 93673\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary.\n",
    "chars = sorted(list(set(all_chars)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(all_chars)\n",
    "n_vocab = len(chars)\n",
    "print('Total characters: %d' % n_chars)\n",
    "print('Total vocab: %d' % n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 46817\n"
     ]
    }
   ],
   "source": [
    "# Construct dataset.\n",
    "seq_len = 40\n",
    "step_size = 2\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_len, step_size):\n",
    "    seq_in = all_chars[i: i + seq_len]\n",
    "    seq_out = all_chars[i + seq_len]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print('Total patterns: %d' % n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_len, 1))\n",
    "X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200)               161600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 169,238\n",
      "Trainable params: 169,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dim = 200\n",
    "model = Sequential()\n",
    "model.add(LSTM(dim, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'lstm_weights/best_model_stepsize2_dim%d.h5' % dim\n",
    "mc = ModelCheckpoint(fn, monitor='acc', save_best_only=True)\n",
    "es = EarlyStopping(monitor='acc', baseline=0.6, patience=0)\n",
    "callbacks_list = [mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46817/46817 [==============================] - 16s 350us/step - loss: 2.9810 - acc: 0.1685\n",
      "Epoch 2/200\n",
      "46817/46817 [==============================] - 16s 345us/step - loss: 2.7869 - acc: 0.2234\n",
      "Epoch 3/200\n",
      "46817/46817 [==============================] - 16s 347us/step - loss: 2.7072 - acc: 0.2401\n",
      "Epoch 4/200\n",
      "46817/46817 [==============================] - 17s 364us/step - loss: 2.6545 - acc: 0.2510\n",
      "Epoch 5/200\n",
      "46817/46817 [==============================] - 17s 357us/step - loss: 2.6101 - acc: 0.2625\n",
      "Epoch 6/200\n",
      "46817/46817 [==============================] - 16s 352us/step - loss: 2.5644 - acc: 0.2683\n",
      "Epoch 7/200\n",
      "46817/46817 [==============================] - 16s 347us/step - loss: 2.5293 - acc: 0.2768\n",
      "Epoch 8/200\n",
      "46817/46817 [==============================] - 16s 350us/step - loss: 2.4975 - acc: 0.2843\n",
      "Epoch 9/200\n",
      "46817/46817 [==============================] - 17s 353us/step - loss: 2.4651 - acc: 0.2875\n",
      "Epoch 10/200\n",
      "46817/46817 [==============================] - 17s 353us/step - loss: 2.4356 - acc: 0.2943\n",
      "Epoch 11/200\n",
      "46817/46817 [==============================] - 17s 356us/step - loss: 2.4073 - acc: 0.3037\n",
      "Epoch 12/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 2.3800 - acc: 0.3123\n",
      "Epoch 13/200\n",
      "46817/46817 [==============================] - 17s 357us/step - loss: 2.3549 - acc: 0.3174\n",
      "Epoch 14/200\n",
      "46817/46817 [==============================] - 17s 365us/step - loss: 2.3259 - acc: 0.3259\n",
      "Epoch 15/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 2.2994 - acc: 0.3327\n",
      "Epoch 16/200\n",
      "46817/46817 [==============================] - 17s 363us/step - loss: 2.2743 - acc: 0.3368\n",
      "Epoch 17/200\n",
      "46817/46817 [==============================] - 17s 358us/step - loss: 2.2500 - acc: 0.3431\n",
      "Epoch 18/200\n",
      "46817/46817 [==============================] - 17s 357us/step - loss: 2.2258 - acc: 0.3497\n",
      "Epoch 19/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 2.1992 - acc: 0.3551\n",
      "Epoch 20/200\n",
      "46817/46817 [==============================] - 17s 357us/step - loss: 2.1747 - acc: 0.3610\n",
      "Epoch 21/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 2.1496 - acc: 0.3682\n",
      "Epoch 22/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 2.1282 - acc: 0.3719\n",
      "Epoch 23/200\n",
      "46817/46817 [==============================] - 17s 359us/step - loss: 2.1028 - acc: 0.3792\n",
      "Epoch 24/200\n",
      "46817/46817 [==============================] - 17s 361us/step - loss: 2.0760 - acc: 0.3863\n",
      "Epoch 25/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 2.0526 - acc: 0.3895\n",
      "Epoch 26/200\n",
      "46817/46817 [==============================] - 17s 365us/step - loss: 2.0328 - acc: 0.3982\n",
      "Epoch 27/200\n",
      "46817/46817 [==============================] - 17s 359us/step - loss: 2.0068 - acc: 0.4026\n",
      "Epoch 28/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 1.9844 - acc: 0.4104\n",
      "Epoch 29/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.9633 - acc: 0.4147\n",
      "Epoch 30/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 1.9424 - acc: 0.4190\n",
      "Epoch 31/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 1.9239 - acc: 0.4225\n",
      "Epoch 32/200\n",
      "46817/46817 [==============================] - 17s 359us/step - loss: 1.9003 - acc: 0.4307\n",
      "Epoch 33/200\n",
      "46817/46817 [==============================] - 17s 363us/step - loss: 1.8821 - acc: 0.4338\n",
      "Epoch 34/200\n",
      "46817/46817 [==============================] - 17s 361us/step - loss: 1.8646 - acc: 0.4404\n",
      "Epoch 35/200\n",
      "46817/46817 [==============================] - 17s 363us/step - loss: 1.8449 - acc: 0.4459\n",
      "Epoch 36/200\n",
      "46817/46817 [==============================] - 17s 361us/step - loss: 1.8250 - acc: 0.4493\n",
      "Epoch 37/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.8220 - acc: 0.4502\n",
      "Epoch 38/200\n",
      "46817/46817 [==============================] - 17s 357us/step - loss: 1.7993 - acc: 0.4545\n",
      "Epoch 39/200\n",
      "46817/46817 [==============================] - 17s 356us/step - loss: 1.7849 - acc: 0.4616\n",
      "Epoch 40/200\n",
      "46817/46817 [==============================] - 17s 359us/step - loss: 1.7626 - acc: 0.4673\n",
      "Epoch 41/200\n",
      "46817/46817 [==============================] - 17s 363us/step - loss: 1.7386 - acc: 0.4744\n",
      "Epoch 42/200\n",
      "46817/46817 [==============================] - 16s 351us/step - loss: 1.7390 - acc: 0.4751\n",
      "Epoch 43/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.7192 - acc: 0.4788\n",
      "Epoch 44/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.7026 - acc: 0.4845\n",
      "Epoch 45/200\n",
      "46817/46817 [==============================] - 17s 365us/step - loss: 1.6937 - acc: 0.4858\n",
      "Epoch 46/200\n",
      "46817/46817 [==============================] - 17s 368us/step - loss: 1.6818 - acc: 0.4892\n",
      "Epoch 47/200\n",
      "46817/46817 [==============================] - 16s 350us/step - loss: 1.6606 - acc: 0.4951\n",
      "Epoch 48/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 1.7104 - acc: 0.4816\n",
      "Epoch 49/200\n",
      "46817/46817 [==============================] - 17s 373us/step - loss: 1.6440 - acc: 0.4996\n",
      "Epoch 50/200\n",
      "46817/46817 [==============================] - 17s 359us/step - loss: 1.6355 - acc: 0.4995\n",
      "Epoch 51/200\n",
      "46817/46817 [==============================] - 16s 351us/step - loss: 1.6270 - acc: 0.5036\n",
      "Epoch 52/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.6068 - acc: 0.5121\n",
      "Epoch 53/200\n",
      "46817/46817 [==============================] - 17s 354us/step - loss: 1.5992 - acc: 0.5116\n",
      "Epoch 54/200\n",
      "46817/46817 [==============================] - 17s 356us/step - loss: 1.5937 - acc: 0.5128\n",
      "Epoch 55/200\n",
      "46817/46817 [==============================] - 17s 361us/step - loss: 1.5912 - acc: 0.5129\n",
      "Epoch 56/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 1.5653 - acc: 0.5235\n",
      "Epoch 57/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.5515 - acc: 0.5272\n",
      "Epoch 58/200\n",
      "46817/46817 [==============================] - 17s 367us/step - loss: 1.5611 - acc: 0.5252\n",
      "Epoch 59/200\n",
      "46817/46817 [==============================] - 17s 356us/step - loss: 1.5367 - acc: 0.5305\n",
      "Epoch 60/200\n",
      "46817/46817 [==============================] - 17s 354us/step - loss: 1.5491 - acc: 0.5249\n",
      "Epoch 61/200\n",
      "46817/46817 [==============================] - 17s 356us/step - loss: 1.5203 - acc: 0.5337\n",
      "Epoch 62/200\n",
      "46817/46817 [==============================] - 17s 366us/step - loss: 1.5198 - acc: 0.5346\n",
      "Epoch 63/200\n",
      "46817/46817 [==============================] - 17s 354us/step - loss: 1.5220 - acc: 0.5342\n",
      "Epoch 64/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.4952 - acc: 0.5418\n",
      "Epoch 65/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.4976 - acc: 0.5405\n",
      "Epoch 66/200\n",
      "46817/46817 [==============================] - 17s 371us/step - loss: 1.5577 - acc: 0.5255\n",
      "Epoch 67/200\n",
      "46817/46817 [==============================] - 17s 358us/step - loss: 1.5183 - acc: 0.5329\n",
      "Epoch 68/200\n",
      "46817/46817 [==============================] - 17s 360us/step - loss: 1.4758 - acc: 0.5495\n",
      "Epoch 69/200\n",
      "46817/46817 [==============================] - 16s 352us/step - loss: 1.4605 - acc: 0.5530\n",
      "Epoch 70/200\n",
      "46817/46817 [==============================] - 17s 367us/step - loss: 1.4466 - acc: 0.5563\n",
      "Epoch 71/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.4600 - acc: 0.5506\n",
      "Epoch 72/200\n",
      "46817/46817 [==============================] - 17s 365us/step - loss: 1.4714 - acc: 0.5468\n",
      "Epoch 73/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.4512 - acc: 0.5549\n",
      "Epoch 74/200\n",
      "46817/46817 [==============================] - 17s 370us/step - loss: 1.4563 - acc: 0.5533\n",
      "Epoch 75/200\n",
      "46817/46817 [==============================] - 18s 375us/step - loss: 1.4218 - acc: 0.5646\n",
      "Epoch 76/200\n",
      "46817/46817 [==============================] - 17s 373us/step - loss: 1.4498 - acc: 0.5536\n",
      "Epoch 77/200\n",
      "46817/46817 [==============================] - 17s 372us/step - loss: 1.4429 - acc: 0.5562\n",
      "Epoch 78/200\n",
      "46817/46817 [==============================] - 18s 386us/step - loss: 1.4211 - acc: 0.5620\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46817/46817 [==============================] - 17s 365us/step - loss: 1.4152 - acc: 0.5641\n",
      "Epoch 80/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.3910 - acc: 0.5712\n",
      "Epoch 81/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.4058 - acc: 0.5672\n",
      "Epoch 82/200\n",
      "46817/46817 [==============================] - 17s 367us/step - loss: 1.4160 - acc: 0.5635\n",
      "Epoch 83/200\n",
      "46817/46817 [==============================] - 18s 374us/step - loss: 1.4523 - acc: 0.5515\n",
      "Epoch 84/200\n",
      "46817/46817 [==============================] - 17s 366us/step - loss: 1.3877 - acc: 0.5722\n",
      "Epoch 85/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.3854 - acc: 0.5725\n",
      "Epoch 86/200\n",
      "46817/46817 [==============================] - 17s 365us/step - loss: 1.3651 - acc: 0.5808\n",
      "Epoch 87/200\n",
      "46817/46817 [==============================] - 17s 361us/step - loss: 1.3501 - acc: 0.5829\n",
      "Epoch 88/200\n",
      "46817/46817 [==============================] - 17s 372us/step - loss: 1.4760 - acc: 0.5453\n",
      "Epoch 89/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.3476 - acc: 0.5824\n",
      "Epoch 90/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.3470 - acc: 0.5848\n",
      "Epoch 91/200\n",
      "46817/46817 [==============================] - 17s 372us/step - loss: 1.3405 - acc: 0.5865\n",
      "Epoch 92/200\n",
      "46817/46817 [==============================] - 17s 370us/step - loss: 1.3354 - acc: 0.5885\n",
      "Epoch 93/200\n",
      "46817/46817 [==============================] - 17s 362us/step - loss: 1.3460 - acc: 0.5828\n",
      "Epoch 94/200\n",
      "46817/46817 [==============================] - 17s 363us/step - loss: 1.3475 - acc: 0.5808\n",
      "Epoch 95/200\n",
      "46817/46817 [==============================] - 17s 370us/step - loss: 1.3575 - acc: 0.5784\n",
      "Epoch 96/200\n",
      "46817/46817 [==============================] - 17s 364us/step - loss: 1.3343 - acc: 0.5870\n",
      "Epoch 97/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.3391 - acc: 0.5852\n",
      "Epoch 98/200\n",
      "46817/46817 [==============================] - 17s 370us/step - loss: 1.3228 - acc: 0.5918\n",
      "Epoch 99/200\n",
      "46817/46817 [==============================] - 17s 367us/step - loss: 1.3115 - acc: 0.5945\n",
      "Epoch 100/200\n",
      "46817/46817 [==============================] - 17s 370us/step - loss: 1.3049 - acc: 0.5961\n",
      "Epoch 101/200\n",
      "46817/46817 [==============================] - 17s 366us/step - loss: 1.3354 - acc: 0.5863\n",
      "Epoch 102/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.2950 - acc: 0.5974\n",
      "Epoch 103/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.3373 - acc: 0.5857\n",
      "Epoch 104/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.3195 - acc: 0.5907\n",
      "Epoch 105/200\n",
      "46817/46817 [==============================] - 17s 371us/step - loss: 1.2823 - acc: 0.6000\n",
      "Epoch 106/200\n",
      "46817/46817 [==============================] - 17s 369us/step - loss: 1.3263 - acc: 0.5908\n",
      "Epoch 107/200\n",
      "46817/46817 [==============================] - 17s 370us/step - loss: 1.3064 - acc: 0.5933\n",
      "Epoch 108/200\n",
      "46817/46817 [==============================] - 18s 374us/step - loss: 1.2854 - acc: 0.6011\n",
      "Epoch 109/200\n",
      "46817/46817 [==============================] - 18s 384us/step - loss: 1.2738 - acc: 0.6038\n",
      "Epoch 110/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.2734 - acc: 0.6049\n",
      "Epoch 111/200\n",
      "46817/46817 [==============================] - 18s 376us/step - loss: 1.2888 - acc: 0.5988\n",
      "Epoch 112/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.3217 - acc: 0.5878\n",
      "Epoch 113/200\n",
      "46817/46817 [==============================] - 18s 385us/step - loss: 1.2580 - acc: 0.6067\n",
      "Epoch 114/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.2628 - acc: 0.6049\n",
      "Epoch 115/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.2767 - acc: 0.6022\n",
      "Epoch 116/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.2495 - acc: 0.6102\n",
      "Epoch 117/200\n",
      "46817/46817 [==============================] - 18s 388us/step - loss: 1.2351 - acc: 0.6156\n",
      "Epoch 118/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.2781 - acc: 0.6027\n",
      "Epoch 119/200\n",
      "46817/46817 [==============================] - 18s 384us/step - loss: 1.2686 - acc: 0.6042\n",
      "Epoch 120/200\n",
      "46817/46817 [==============================] - 18s 395us/step - loss: 1.2600 - acc: 0.6054\n",
      "Epoch 121/200\n",
      "46817/46817 [==============================] - 18s 391us/step - loss: 1.2909 - acc: 0.5992\n",
      "Epoch 122/200\n",
      "46817/46817 [==============================] - 18s 376us/step - loss: 1.2431 - acc: 0.6134\n",
      "Epoch 123/200\n",
      "46817/46817 [==============================] - 18s 392us/step - loss: 1.2297 - acc: 0.6158\n",
      "Epoch 124/200\n",
      "46817/46817 [==============================] - 19s 396us/step - loss: 1.2251 - acc: 0.6191\n",
      "Epoch 125/200\n",
      "46817/46817 [==============================] - 18s 386us/step - loss: 1.2404 - acc: 0.6145\n",
      "Epoch 126/200\n",
      "46817/46817 [==============================] - 18s 389us/step - loss: 1.2340 - acc: 0.6142\n",
      "Epoch 127/200\n",
      "46817/46817 [==============================] - 18s 389us/step - loss: 1.3225 - acc: 0.5873\n",
      "Epoch 128/200\n",
      "46817/46817 [==============================] - 18s 391us/step - loss: 1.2502 - acc: 0.6089\n",
      "Epoch 129/200\n",
      "46817/46817 [==============================] - 19s 401us/step - loss: 1.2540 - acc: 0.6082\n",
      "Epoch 130/200\n",
      "46817/46817 [==============================] - 18s 393us/step - loss: 1.2251 - acc: 0.6177\n",
      "Epoch 131/200\n",
      "46817/46817 [==============================] - 18s 393us/step - loss: 1.2202 - acc: 0.6202\n",
      "Epoch 132/200\n",
      "46817/46817 [==============================] - 19s 396us/step - loss: 1.2026 - acc: 0.6215\n",
      "Epoch 133/200\n",
      "46817/46817 [==============================] - 18s 394us/step - loss: 1.1733 - acc: 0.6351\n",
      "Epoch 134/200\n",
      "46817/46817 [==============================] - 18s 395us/step - loss: 1.1867 - acc: 0.6289\n",
      "Epoch 135/200\n",
      "46817/46817 [==============================] - 19s 402us/step - loss: 1.2254 - acc: 0.6151\n",
      "Epoch 136/200\n",
      "46817/46817 [==============================] - 19s 397us/step - loss: 1.1838 - acc: 0.6306\n",
      "Epoch 137/200\n",
      "46817/46817 [==============================] - 19s 412us/step - loss: 1.2233 - acc: 0.6182\n",
      "Epoch 138/200\n",
      "46817/46817 [==============================] - 19s 404us/step - loss: 1.2223 - acc: 0.6169\n",
      "Epoch 139/200\n",
      "46817/46817 [==============================] - 18s 386us/step - loss: 1.1923 - acc: 0.6290\n",
      "Epoch 140/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.1793 - acc: 0.6311\n",
      "Epoch 141/200\n",
      "46817/46817 [==============================] - 18s 384us/step - loss: 1.2310 - acc: 0.6130\n",
      "Epoch 142/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.2056 - acc: 0.6193\n",
      "Epoch 143/200\n",
      "46817/46817 [==============================] - 18s 376us/step - loss: 1.1824 - acc: 0.6308\n",
      "Epoch 144/200\n",
      "46817/46817 [==============================] - 18s 387us/step - loss: 1.1586 - acc: 0.6392\n",
      "Epoch 145/200\n",
      "46817/46817 [==============================] - 17s 372us/step - loss: 1.1620 - acc: 0.6367\n",
      "Epoch 146/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.1311 - acc: 0.6453\n",
      "Epoch 147/200\n",
      "46817/46817 [==============================] - 18s 387us/step - loss: 1.1793 - acc: 0.6313\n",
      "Epoch 148/200\n",
      "46817/46817 [==============================] - 18s 381us/step - loss: 1.1645 - acc: 0.6368\n",
      "Epoch 149/200\n",
      "46817/46817 [==============================] - 18s 379us/step - loss: 1.2013 - acc: 0.6261\n",
      "Epoch 150/200\n",
      "46817/46817 [==============================] - 18s 390us/step - loss: 1.1719 - acc: 0.6344\n",
      "Epoch 151/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.3016 - acc: 0.5941\n",
      "Epoch 152/200\n",
      "46817/46817 [==============================] - 17s 373us/step - loss: 1.2086 - acc: 0.6228\n",
      "Epoch 153/200\n",
      "46817/46817 [==============================] - 18s 389us/step - loss: 1.1743 - acc: 0.6305\n",
      "Epoch 154/200\n",
      "46817/46817 [==============================] - 18s 382us/step - loss: 1.1707 - acc: 0.6334\n",
      "Epoch 155/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.1572 - acc: 0.6387\n",
      "Epoch 156/200\n",
      "46817/46817 [==============================] - 18s 389us/step - loss: 1.1795 - acc: 0.6280\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46817/46817 [==============================] - 18s 379us/step - loss: 1.1444 - acc: 0.6404\n",
      "Epoch 158/200\n",
      "46817/46817 [==============================] - 18s 384us/step - loss: 1.1226 - acc: 0.6487\n",
      "Epoch 159/200\n",
      "46817/46817 [==============================] - 18s 379us/step - loss: 1.1697 - acc: 0.6326\n",
      "Epoch 160/200\n",
      "46817/46817 [==============================] - 18s 392us/step - loss: 1.2593 - acc: 0.6060\n",
      "Epoch 161/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.1644 - acc: 0.6326\n",
      "Epoch 162/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.1454 - acc: 0.6410\n",
      "Epoch 163/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.1445 - acc: 0.6393\n",
      "Epoch 164/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.1545 - acc: 0.6387\n",
      "Epoch 165/200\n",
      "46817/46817 [==============================] - 18s 376us/step - loss: 1.1322 - acc: 0.6399\n",
      "Epoch 166/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.1570 - acc: 0.6374\n",
      "Epoch 167/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.1117 - acc: 0.6479\n",
      "Epoch 168/200\n",
      "46817/46817 [==============================] - 18s 379us/step - loss: 1.1611 - acc: 0.6365\n",
      "Epoch 169/200\n",
      "46817/46817 [==============================] - 18s 382us/step - loss: 1.1536 - acc: 0.6384\n",
      "Epoch 170/200\n",
      "46817/46817 [==============================] - 18s 387us/step - loss: 1.1194 - acc: 0.6485\n",
      "Epoch 171/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.1259 - acc: 0.6440\n",
      "Epoch 172/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.1106 - acc: 0.6488\n",
      "Epoch 173/200\n",
      "46817/46817 [==============================] - 18s 388us/step - loss: 1.0771 - acc: 0.6605\n",
      "Epoch 174/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.1020 - acc: 0.6545\n",
      "Epoch 175/200\n",
      "46817/46817 [==============================] - 18s 380us/step - loss: 1.2696 - acc: 0.6073\n",
      "Epoch 176/200\n",
      "46817/46817 [==============================] - 18s 378us/step - loss: 1.2565 - acc: 0.6060\n",
      "Epoch 177/200\n",
      "46817/46817 [==============================] - 18s 384us/step - loss: 1.1407 - acc: 0.6407\n",
      "Epoch 178/200\n",
      "46817/46817 [==============================] - 18s 377us/step - loss: 1.1505 - acc: 0.6364\n",
      "Epoch 179/200\n",
      "46817/46817 [==============================] - 18s 382us/step - loss: 1.1254 - acc: 0.6455\n",
      "Epoch 180/200\n",
      "46817/46817 [==============================] - 18s 388us/step - loss: 1.3869 - acc: 0.5745\n",
      "Epoch 181/200\n",
      "46817/46817 [==============================] - 18s 383us/step - loss: 1.1494 - acc: 0.6396\n",
      "Epoch 182/200\n",
      "46817/46817 [==============================] - 18s 390us/step - loss: 1.1215 - acc: 0.6470\n",
      "Epoch 183/200\n",
      "46817/46817 [==============================] - 18s 390us/step - loss: 1.1071 - acc: 0.6530\n",
      "Epoch 184/200\n",
      "46817/46817 [==============================] - 18s 386us/step - loss: 1.1156 - acc: 0.6502\n",
      "Epoch 185/200\n",
      "46817/46817 [==============================] - 18s 394us/step - loss: 1.1219 - acc: 0.6463\n",
      "Epoch 186/200\n",
      "46817/46817 [==============================] - 18s 388us/step - loss: 1.1239 - acc: 0.6470\n",
      "Epoch 187/200\n",
      "46817/46817 [==============================] - 18s 391us/step - loss: 1.3916 - acc: 0.5765\n",
      "Epoch 188/200\n",
      "46817/46817 [==============================] - 19s 401us/step - loss: 1.1137 - acc: 0.6492\n",
      "Epoch 189/200\n",
      "46817/46817 [==============================] - 18s 389us/step - loss: 1.0773 - acc: 0.6616\n",
      "Epoch 190/200\n",
      "46817/46817 [==============================] - 18s 394us/step - loss: 1.0604 - acc: 0.6677\n",
      "Epoch 191/200\n",
      "46817/46817 [==============================] - 18s 393us/step - loss: 1.0528 - acc: 0.6684\n",
      "Epoch 192/200\n",
      "46817/46817 [==============================] - 19s 400us/step - loss: 1.1394 - acc: 0.6396\n",
      "Epoch 193/200\n",
      "46817/46817 [==============================] - 19s 400us/step - loss: 1.0665 - acc: 0.6644\n",
      "Epoch 194/200\n",
      "46817/46817 [==============================] - 19s 412us/step - loss: 1.1235 - acc: 0.6451\n",
      "Epoch 195/200\n",
      "46817/46817 [==============================] - 19s 404us/step - loss: 1.0942 - acc: 0.6533\n",
      "Epoch 196/200\n",
      "46817/46817 [==============================] - 19s 399us/step - loss: 1.0639 - acc: 0.66471s - loss: 1.0567\n",
      "Epoch 197/200\n",
      "46817/46817 [==============================] - 19s 398us/step - loss: 1.0539 - acc: 0.6688\n",
      "Epoch 198/200\n",
      "46817/46817 [==============================] - 18s 387us/step - loss: 1.1365 - acc: 0.6415\n",
      "Epoch 199/200\n",
      "46817/46817 [==============================] - 18s 390us/step - loss: 1.1185 - acc: 0.6488\n",
      "Epoch 200/200\n",
      "46817/46817 [==============================] - 18s 393us/step - loss: 1.0715 - acc: 0.6629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7e2837b518>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # Helper function to sample from a probability array\n",
    "    # Source: https://keras.io/examples/lstm_text_generation/\n",
    "    preds = np.exp(preds / temperature)\n",
    "    probs = preds / np.sum(np.exp(preds / temperature))\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46817/46817 [==============================] - 12s 246us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0193825700311212, 0.6822949051856995]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 200\n",
    "ckpt = \"lstm_weights/best_model_stepsize2_dim%d.h5\" % dim\n",
    "model.load_weights(ckpt)\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "pattern = [char_to_int[char] for char in seed]\n",
    "output_seq = pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 1.5\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)[0]\n",
    "    index = sample(prediction, temperature=tmp)\n",
    "    #index = np.argmax(prediction)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:]\n",
    "    output_seq.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "tthou hrsw pong,\n",
      "and a map d nonw that woue dx the gail,\n",
      "if thou ae wtatthngd mine sha lise eaik uhich:\n",
      "bu aly she mot my lifd of my love dossont dnrh.\n",
      "that you tealo the farhe with hare iu blate,\n",
      "andcl io ios shat which fote lo not sooe?\n",
      "mo eosh midht that thay fole cace mo jove,\n",
      "shat you were uourhy sooe vorthp ther des,\n",
      "whth niny the whrd poopui pow my lofw.\n",
      "hoo thened iof nfres to the meny sr me.\n",
      "ty ferd io my aeduty lole blltert mf,\n",
      "howes,heented wo tey,st ieve wet the foowi,\n",
      "and the wrrld wou dearying oo the werd roeie\n",
      "to mesorreng afaio hv seyse shat tie soaes,\n",
      "gor the vorld soon gym to she menhlsg nrn,\n",
      "yhth all the mars,\n",
      "and a gaievu sie oor ro shmngsed to woy.\n",
      "and eea\n",
      "oyt opw by the vhil,\n",
      "wher i bm cosh iy she menhsing thee,\n",
      "and iaale yotr cruk ountese pf thi thahnu\n",
      "than thay forwu, and av a sart,\n",
      "the eoos hnv jovesthng goaee,\n",
      "oor rrou lookeng worthy moow brofkrs fedr,\n",
      "and the whrd pfrppnee eyes hot shy wonklf:\n",
      "no eesties thene iarh iiv powrhry ptiased.\n",
      "thy bla ooshsn pf thy se\n"
     ]
    }
   ],
   "source": [
    "output = [int_to_char[i] for i in output_seq]\n",
    "output = ''.join(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
